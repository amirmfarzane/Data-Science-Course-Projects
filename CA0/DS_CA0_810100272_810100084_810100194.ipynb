{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648ac049",
   "metadata": {},
   "source": [
    "# <html>\n",
    "   \n",
    "<div style=\"background-image: linear-gradient(to left, rgb(255, 255, 255), rgb(138, 136, 136)); width: auto; margin: 10px;\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/f/fd/University_of_Tehran_logo.svg/225px-University_of_Tehran_logo.svg.png\" width=100px width=auto style=\"padding:10px; vertical-align: center;\">\n",
    " \n",
    "</div>\n",
    "   \n",
    "<div   style:\"text-align: center; background-image: linear-gradient(to left, rgb(255, 255, 255), rgb( 219, 204, 245  ));width: 400px; height: 30px; \">\n",
    "<h1 style=\"font-family: Georgia; color: black; text-align: center; \">Course: Data Science </h1>\n",
    "\n",
    "</div>\n",
    "    <div   style:\"border: 3px solid green;text-align: center; \">\n",
    "<h1 style=\"font-family: Georgia; color: black; text-align: center; \">CA0-Introduction to Data Science</h1>\n",
    "        <h1 style=\"font-family: Georgia; color: black; text-align: center; \">Team memebers:</h1>\n",
    "\n",
    "</div>\n",
    "\n",
    "   <div>    \n",
    "<h1 style=\"font-family: Georgia; color: black; text-align: center; font-size:15px;\">Shahzad Momayez-sid:810100272 </h1>\n",
    "       <h1 style=\"font-family: Georgia; color: black; text-align: center; font-size:15px;\">Mohammad Amanlou-sid:810100084 </h1>\n",
    "       <h1 style=\"font-family: Georgia; color: black; text-align: center; font-size:15px;\">Amir Mahdi Farzaneh-sid:810100194 </h1>\n",
    "\n",
    "</div>\n",
    "   \n",
    "\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e6b10",
   "metadata": {},
   "source": [
    "## The purpose of the assignment:\n",
    "the purpose of this assignment was web scraping and conducting basic statistical analysis.\n",
    "\n",
    "## A brief summary of what we did:\n",
    "actually, in order to complete this project we had to do 3 tasks:\n",
    "  - 1. Environment Setup: Install the required libraries such as Beautiful Soup, Selenium,\n",
    "pandas, numpy, matplotlib, and seaborn.\n",
    "\n",
    "  - 2. Web Scraping: Write a script to scrape transaction data from Etherscan.io. Use\n",
    "Selenium to interact with the website and Beautiful Soup to parse the HTML content.\n",
    "\n",
    "  - 3. Data Sampling and Analysis: Once the data is collected, create a sample from the\n",
    "dataset. Compare the sample statistics (mean and standard deviation) with the\n",
    "population statistics.\n",
    "\n",
    "## An explanation for each dataset that is loaded in the notebook, why youâ€™re using it, is it for training process or test, etc:\n",
    "well, first of all we used web scraping in order to extract the data from the website. and we saved all data into a CSV and the showed it as a df named \"df_optimized\". In this project we did not need to divide the data into train and test!\n",
    "we used web scraping to gather transaction data from the Ethereum blockchain using the Etherscan block explorer. Our objective was to collect transactions from the last 10 blocks on Ethereum. To accomplish this task, we will employ web scraping\n",
    "techniques to extract the transaction data from the Etherscan website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfbb4b0c704e67",
   "metadata": {},
   "source": [
    "# Web Scraping and Introductory Data Analysis\n",
    "\n",
    "Welcome to Homework 0, where we will delve into web scraping and perform an introductory data analysis. This homework will be a hands-on exercise that will help you become familiar with the process of extracting data from websites and conducting basic statistical analysis. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this homework, you will be able to:\n",
    "\n",
    "1. Set up a Python environment with the necessary libraries for web scraping and data analysis.\n",
    "2. Write a web scraping script using Beautiful Soup and Selenium to collect data from a website.\n",
    "3. Sample from the collected dataset and compare the statistics of the sample and the population.\n",
    "   \n",
    "## Tasks\n",
    "\n",
    "1. **Environment Setup**: Install the required libraries such as Beautiful Soup, Selenium, pandas, numpy, matplotlib, and seaborn.\n",
    "\n",
    "2. **Web Scraping**: Write a script to scrape transaction data from [Etherscan.io](https://etherscan.io/txs). Use Selenium to interact with the website and Beautiful Soup to parse the HTML content.\n",
    "\n",
    "3. **Data Sampling**: Once the data is collected, create a sample from the dataset. Compare the sample statistics (mean and standard deviation) with the population statistics.\n",
    "\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. A Jupyter notebook with all the code and explanations.\n",
    "2. A detailed report on the findings, including the comparison of sample and population statistics.\n",
    "Note: You can include the report in your notebook.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Begin by setting up your Python environment and installing the necessary libraries. Then, proceed with the web scraping task, ensuring that you handle any potential issues such as rate limiting. Once you have the data, move on to the data sampling and statistical analysis tasks. \n",
    "\n",
    "Remember to document your process and findings in the Jupyter notebook, and to include visualizations where appropriate to illustrate your results. <br>\n",
    "Good luck, and happy scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca352a49724d191",
   "metadata": {},
   "source": [
    "## Data Collection (Etherscan)\n",
    "\n",
    "In this section, we will use web scraping to gather transaction data from the Ethereum blockchain using the Etherscan block explorer. Our objective is to collect transactions from the **last 10 blocks** on Ethereum.\n",
    "\n",
    "To accomplish this task, we will employ web scraping techniques to extract the transaction data from the Etherscan website. The URL we will be targeting for our data collection is:\n",
    "\n",
    "[https://etherscan.io/txs](https://etherscan.io/txs)\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Navigate to the URL**: Use Selenium to open the Etherscan transactions page in a browser.\n",
    "\n",
    "2. **Locate the Transaction Data**: Identify the HTML elements that contain the transaction data for the specified block range.\n",
    "\n",
    "3. **Extract the Data**: Write a script to extract the transaction details e.g. Hash, Method, Block, etc.\n",
    "\n",
    "4. **Handle Pagination**: If the transactions span multiple pages, implement pagination handling to navigate through the pages and collect all relevant transaction data.\n",
    "\n",
    "5. **Store the Data**: Save the extracted transaction data into a structured format, such as a CSV file or a pandas DataFrame, for further analysis.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Rate Limiting**: Be mindful of the website's rate limits to avoid being blocked. Implement delays between requests if necessary.\n",
    "- **Dynamic Content**: The Etherscan website may load content dynamically. Ensure that Selenium waits for the necessary elements to load before attempting to scrape the data.\n",
    "- **Data Cleaning**: After extraction, clean the data to remove any inconsistencies or errors that may have occurred during the scraping process.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Selenium Documentation](https://selenium-python.readthedocs.io/)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Ethereum](https://ethereum.org/en/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fa10db-ec9e-4921-870a-50066926ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC  \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2232f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_data():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "\n",
    "    # Open Etherscan\n",
    "    driver.get(\"https://etherscan.io/blocks\")\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    table = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'table')))\n",
    "   \n",
    "\n",
    "    # Use BeautifulSoup to parse block numbers\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    blocks = [row.find('a').text for row in soup.find_all('tr')[1:11]]  # Get the last 10 blocks\n",
    "\n",
    "    transactions_data = []\n",
    "\n",
    "    # Iterate over blocks\n",
    "    # extract transactions\n",
    "    for block in blocks:\n",
    "        driver.get(f\"https://etherscan.io/txs?block={block}\")\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'table-hover')))\n",
    "\n",
    "        # iterate over pagination\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        pagination = soup.find('ul', class_='pagination')\n",
    "        pages = 1\n",
    "        if pagination:\n",
    "            pages = len(pagination.find_all('li')) - 2  # Adjust for 'previous' and 'next' buttons\n",
    "\n",
    "        for page in range(1, pages + 1):\n",
    "            if page > 1:\n",
    "                # Go directly to the page\n",
    "                driver.get(f\"https://etherscan.io/txs?block={block}&p={page}\")\n",
    "                wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'table-hover')))\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            transactions_table = soup.find('table', class_='table-hover')\n",
    "\n",
    "            # Extract transactions\n",
    "            for row in transactions_table.find('tbody').find_all('tr'):\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) > 1:  # Ensure it's not an empty row\n",
    "                    transaction = {\n",
    "                        'Hash': cells[1].text.strip(),\n",
    "                        'Method': cells[2].text.strip(),\n",
    "                        'Block': block,\n",
    "                        'Age': cells[4].text.strip(),\n",
    "                        'From': cells[7].text.strip(),\n",
    "                        'To': cells[9].text.strip(),\n",
    "                        'Value': cells[10].text.strip(),\n",
    "                        'TxnFee': cells[11].text.strip(),\n",
    "                    }\n",
    "                    transactions_data.append(transaction)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame and save as CSV\n",
    "    df = pd.DataFrame(transactions_data)\n",
    "    df.to_csv('data.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Execute the function and print the first few rows of the dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b530b62-1c42-499d-89cf-f60e107bdbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimized = scrape_data()\n",
    "print(df_optimized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimized.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a013b104d142cfc",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Now that we have collected the transaction data from Etherscan, the next step is to perform conduct an initial analysis. This task will involve the following steps:\n",
    "\n",
    "1. **Load the Data**: Import the collected transaction data into a pandas DataFrame.\n",
    "\n",
    "2. **Data Cleaning**: Clean the data by converting data types, removing any irrelevant information, and handling **duplicate** values.\n",
    "\n",
    "3. **Statistical Analysis**: Calculate the mean and standard deviation of the population. Evaluate these statistics to understand the distribution of transaction values. The analysis and plotting will be on **Txn Fee** and **Value**.\n",
    "\n",
    "4. **Visualization**: This phase involves the creation of visual representations to aid in the analysis of transaction values. The visualizations include:\n",
    "    - A histogram for each data column, which provides a visual representation of the data distribution. The selection of bin size is crucial and should be based on the data's characteristics to ensure accurate representation. Provide an explanation on the bin size selection!\n",
    "    - A normal distribution plot fitted alongside the histogram to compare the empirical distribution of the data with the theoretical normal distribution.\n",
    "    - A box plot and a violin plot to identify outliers and provide a comprehensive view of the data's distribution.\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "The project aims to deliver the following deliverables:\n",
    "\n",
    "- A refined pandas DataFrame containing the transaction data, which has undergone thorough cleaning and is ready for analysis.\n",
    "- A simple statistical analysis evaluating the population statistics, offering insights into the distribution of transaction values and fees.\n",
    "- A set of visualizations showcasing the distribution of transaction values for the population. These visualizations include histograms, normal distribution plots, box plots, and violin plots, each serving a specific purpose in the analysis.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "The project starts with the importing of transaction data into a pandas DataFrame, setting the stage for data manipulation and analysis. Subsequent steps involve the cleaning of the data to ensure its quality and reliability. Followed by the calculation of population statistics. Finally, a series of visualizations are created to visually analyze the distribution of transaction values and fees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d28219-2c92-4185-a501-37b92ca87cdf",
   "metadata": {},
   "source": [
    "- Remove irrevilant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86920a-870b-4b0f-825e-976167109eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimized.drop(['Method', 'Block', 'Age', 'From',\t'To'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ff4b2-e238-41b5-b47e-13470d553bfd",
   "metadata": {},
   "source": [
    "- Drop duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973fa43-6294-4443-b3e1-1945e6339312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_duplicates = df_optimized.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271dad50-3b77-4c28-be11-8dfd4fbd2478",
   "metadata": {},
   "source": [
    "- Change columns' data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b3bbc-2203-47b0-ac8b-84c5ebad40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_duplicates['Value'] = df_without_duplicates['Value'].str.extract(r'(\\d+\\.\\d+)').astype(float)\n",
    "df_without_duplicates['Value'] = df_without_duplicates['Value'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c449bda-b51c-4fc0-816f-d9c79a4b3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_duplicates['TxnFee'] = df_without_duplicates['TxnFee'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_duplicates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a015b2-2071-45b2-b073-e6cd6f9e737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Value = df_without_duplicates['Value']\n",
    "df_TxnFee = df_without_duplicates['TxnFee']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485fc01-09b1-4343-a33a-c62b59e7a582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T14:02:12.152030482Z",
     "start_time": "2024-02-25T14:02:12.101846096Z"
    }
   },
   "source": [
    "- Unique values number for TxnFee and Value columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ce338-cd9e-4dbb-afe4-cd4d63e2b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique values in Value column : \", df_Value.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73c864-06b2-46cf-83cb-b6ce4f5f18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique values in Value column : \", df_TxnFee.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6053b534-6d50-4514-8765-977dfebc2528",
   "metadata": {},
   "source": [
    "- Number of zeros for Value and TxnFee columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e774a16-6fb9-46c5-af6e-a61ddac883cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of zero Value column : \", (df_Value == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b2ad0-c676-42a3-bdaf-c0f15839d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of zero TxnFee column : \", (df_TxnFee == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c0579-6f0e-4bf7-be67-627ca56e303a",
   "metadata": {},
   "source": [
    "- Mean and Std for Value and TxnFee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aff53e-a1ba-49ce-b15a-e23a1024a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of Value column : \", df_Value.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce52e36-8cd7-42d3-8367-26b3241b3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Std of Value column : \", df_Value.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e2e51-0738-4163-970a-4239ca5fea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of TxnFee column : \", df_TxnFee.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683492ef-1653-4061-a5af-28ed2cda7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Std of TxnFee column : \", df_TxnFee.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353628b-2615-4e7b-ade3-787e4b002b8a",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TxnFree has high standard deviation because <span style=\"color:red\">high number of zero</span> values.\n",
    "\n",
    "Value column standard deviation is low and shows indicates that the values are dense. \n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228d2de-9fc4-4002-9836-316925343f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Value_without_zero = df_without_duplicates[df_without_duplicates['Value'] != 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5655f-2a4f-47b8-83a4-aa6400de077b",
   "metadata": {},
   "source": [
    "- bin width calculated based on approach of this article\n",
    "- \n",
    "https://www.fmrib.ox.ac.uk/datasets/techrep/tr00mj2/tr00mj2/node24.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d417d-6253-4165-bba2-f330117a66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bin_size(df):\n",
    "     return int(((max(df) - min(df))/(pow(len(df), -1/3) * 3.49 * (df.std())) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085330b-53f7-4c73-82ae-1da2a2a8a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = find_bin_size(df_Value_without_zero['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdaed89-1551-44f2-bed6-5ebd14dcf5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50399baf-fe0d-418c-bd84-7812c7d4919e",
   "metadata": {},
   "source": [
    "- Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0555f-f556-4833-96e1-16523cf6d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Value_without_zero['Value'].plot(kind='hist', bins=(bin_size), density=False, alpha=0.5, label='Histogram of Value', grid=True, \n",
    "                                    title = \"Histogram Value without zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455a519-cc42-4e65-a4d1-a0b8042d3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_Value_without_zero[df_Value_without_zero['Value'] < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82790d-878d-4f31-be26-648f1c9774c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"lenght deleted samples with value more than 1 : {} \".format(len(df_Value_without_zero) - len(df2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c57ddb-4ac4-4e4a-ad3a-2ac43aed818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Value'].plot(kind='hist', bins=(20), density=False, alpha=0.5, label='Histogram of Value', grid=True, title = \"Histogram Value less than one and without zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cdd42-2aa9-42c3-96b2-065c0b5e805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Value_without_zero = df_without_duplicates[df_without_duplicates['Value'] != 0] \n",
    "df = df_Value_without_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc06dcb-8691-4e49-9708-6dfa9e63ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TxnFee'].plot(kind='hist', bins=(50), density=True, alpha=0.5 , label='Histogram of TxnFee', grid=True, title = \"Histogram TxnFee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9383918-1542-4a8b-89aa-b253e9206f3e",
   "metadata": {},
   "source": [
    "- Binsize selction method\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f0c7f-e609-4c36-ac7e-66c07e92532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def hist_with_normal(df, title):\n",
    "    bin_size = find_bin_size(df)\n",
    "    df.plot(kind='hist', bins=bin_size, density=True, alpha=0.5, label='Histogram')\n",
    "    # Parameters for the normal distribution (mean and standard deviation)\n",
    "    mu, sigma = df.mean(), df.std()\n",
    "    \n",
    "    # Create a range of x values for the curve\n",
    "    avg_min_max = (df.max() + df.min())/2\n",
    "    x_range = np.linspace( -df.min()-avg_min_max, df.max()+avg_min_max, 1000)\n",
    "    \n",
    "    # Compute the PDF of the normal distribution\n",
    "    pdf_values = norm.pdf(x_range, mu, sigma)\n",
    "    \n",
    "    # Plot the normal distribution curve\n",
    "    plt.plot(x_range, pdf_values, color='red', label='Normal Distribution')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Data Values')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0eb3f-6c5c-4348-b077-ff1e9b0bb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_with_normal(df_Value_without_zero['TxnFee'], \"Histogram TxnFee with Normal distribution \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4379d9d-85b7-4174-a8d7-ade609b11fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_with_normal(df_Value_without_zero['Value'], \"Histogram Value with Normal distribution \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e618417-c1b4-4a65-a84c-4c5db9eb4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_with_normal(df2['Value'], \"Histogram Value less than 1 with Normal distribution \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddb52a-206f-483f-9055-6c8ea675e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['Value']  < 0.1]\n",
    "hist_with_normal(df3['Value'], \"Histogram Value less than 0.1 with Normal distribution \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39253032-9b4d-4dce-8dee-f6286c7d1116",
   "metadata": {},
   "source": [
    "- Analysis\n",
    "\n",
    "As we see in above plots , population has skew to the right . but when we extract values near zero(dense part of samples) we can see , distributionis near to gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a11e1f-ab71-46d6-ba9b-7879a6f92d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Value_without_zero.boxplot(column = 'Value')\n",
    "plt.title(\"Value boxplot | value without zero\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd764d65-79ff-4e34-bfd6-608bd3cec829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.boxplot(column = 'Value')\n",
    "plt.title(\"Value boxplot | value less than 1\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc90124-f9ea-4620-9ad4-2fe66a2eed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.boxplot(column = 'Value')\n",
    "plt.title(\"Value boxplot | value less than 0.1\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3e132-0a45-4a2b-9965-334b9ca1d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_Value_without_zero.boxplot(column = 'TxnFee')\n",
    "plt.title(\"TxnFee boxplot\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd8a62-357f-42b7-8aeb-ea323003ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tnx_1 = df_Value_without_zero[df_Value_without_zero['TxnFee'] < 0.005]\n",
    "df_tnx_1.boxplot(column = 'TxnFee')\n",
    "plt.title(\"TxnFee boxplot | TxnFee less than 0.05\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90185b69-4a90-468f-b7e5-642e667db23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(dataset=df_Value_without_zero['Value'], vert=False)\n",
    "plt.title(\"Value violin | value without zero\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959e3cc-16a6-4068-a982-ada77c5cd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(dataset=df2['Value'], vert=False)\n",
    "plt.title(\"Value violinplot | value less than 1\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee2586-1e2a-4a1b-b872-bb62dad63f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(dataset=df3['Value'], vert=False)\n",
    "plt.title(\"Value violinplot | value less than 0.1\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba170a-87fb-4c52-bea0-7af0351d2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(dataset=df_Value_without_zero['TxnFee'], vert=False)\n",
    "plt.title(\"TxnFee violin | value without zero\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04d6db-6e4f-4d17-9028-4a66db5907a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "plt.violinplot(dataset=df_tnx_1['TxnFee'], vert=False)\n",
    "plt.title(\"TxnFee violin | TxnFee less than 0.05\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1af233-aab9-4092-86eb-e86ea754cdf9",
   "metadata": {},
   "source": [
    "- Analysis\n",
    "  A lot of outliers are in main population but after checking values near zero we see distribution is more similar to normal distribution\n",
    "\n",
    "  Outliers show some pepole has better status and are unique in these samples but typical transactions are similar to normal(without outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87030e5e0b4fe1e6",
   "metadata": {},
   "source": [
    "## Data Sampling and Analysis\n",
    "\n",
    "In this section, we will delve into the process of data sampling and perform an initial analysis on the transaction data we have collected. Our objective is to understand the distribution of transaction values by sampling the data and comparing the sample statistics with the population statistics.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Load the Data**: Import the collected transaction data into a pandas DataFrame.\n",
    "\n",
    "2. **Data Cleaning**: Clean the data by handling missing values, converting data types, and removing any irrelevant information.\n",
    "\n",
    "3. **Simple Random Sampling (SRS)**: Create a sample from the dataset using a simple random sampling method. This involves randomly selecting a subset of the data without regard to any specific characteristics of the data.\n",
    "\n",
    "4. **Stratified Sampling**: Create another sample from the dataset using a stratified sampling method. This involves dividing the data into strata based on a specific characteristic (e.g., transaction value) and then randomly selecting samples from each stratum. Explain what you have stratified the data by and why you chose this column.\n",
    "\n",
    "5. **Statistical Analysis**: Calculate the mean and standard deviation of the samples and the population. Compare these statistics to understand the distribution of transaction values.\n",
    "\n",
    "6. **Visualization**: Plot the distribution of transaction values and fees for both the samples and the population to visually compare their distributions.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Sample Size**: The size of the sample should be large enough to represent the population accurately but not so large that it becomes impractical to analyze.\n",
    "- **Sampling Method**: Choose the appropriate sampling method based on the characteristics of the data and the research question.\n",
    "\n",
    "Explain the above considerations in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a5d09",
   "metadata": {},
   "source": [
    "# part3:\n",
    "\n",
    "### SRS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_without_duplicates))\n",
    "\n",
    "df = pd.DataFrame(df_without_duplicates)\n",
    "\n",
    "# Now, let's apply Simple Random Sampling to select, for example, 20 samples from this DataFrame\n",
    "sample_size = 20\n",
    "df_sample = df.sample(n=sample_size)\n",
    "\n",
    "print(len(df_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e4153-d673-4e05-b651-8ebe56227fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean for SRS Transaction value :  \" ,df_sample['Value'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e0e3a-e88d-418d-94e8-3dd6211c447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean for SRS Transaction TxnFee :  \" ,df_sample['TxnFee'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7144771-0322-4616-8522-a66cda2556de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard deviation for SRS Transaction value :  \" ,df_sample['Value'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f0c97-1d9d-410e-a56f-d184ec26a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard deviation for SRS TxnFee :  \" ,df_sample['TxnFee'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fb118-249e-43aa-85ea-745e66db0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['TxnFee'].plot(kind='hist', bins=find_bin_size(df_sample['TxnFee']), density=False, alpha=0.5, label='Histogram of TxnFee', grid=True, \n",
    "                                    title = \"Histogram of TxnFee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3adf2-87c4-4c18-bdcd-dea23768f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['Value'].plot(kind='hist', bins=find_bin_size(df_sample['Value']), density=False, alpha=0.5, label='Histogram of Value', grid=True, \n",
    "                                    title = \"Histogram of Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb20da6",
   "metadata": {},
   "source": [
    "# Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [0, 20, 40, 60, 80, 100]\n",
    "bin_edges = np.percentile(df['TxnFee'], percentiles)\n",
    "\n",
    "df['stratum'] = pd.cut(df['TxnFee'], bins=bin_edges, include_lowest=True, labels=False)\n",
    "\n",
    "sample_size_per_stratum = 5 \n",
    "stratified_sample = df.groupby('stratum', group_keys=False).apply(lambda x: x.sample(min(len(x), sample_size_per_stratum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f97bcf-9c9a-4e02-a18e-6bb9187bc881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Mean of TxnFee\" , stratified_sample['TxnFee'].mean())\n",
    "print( \"Std of TxnFee\" , stratified_sample['TxnFee'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot for Stratified Samples using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['stratum'] = df['stratum'].astype('category')  # Ensuring 'stratum' is treated as a categorical variable\n",
    "stratified_sample.boxplot(column='TxnFee', by='stratum')\n",
    "plt.title('TxnFee paratmeter Box Plot of Stratified Sample Values by Stratum')\n",
    "plt.suptitle('')  # Remove the default 'Title' subtitle to clean up the plot title\n",
    "plt.xlabel('Stratum')\n",
    "plt.ylabel('TxnFee')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbecbe6e-b4c8-48e2-8950-5fb16de45236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TxnFee'].plot(kind='hist', bins=(find_bin_size(df['TxnFee'])), density=False, alpha=0.5, label='Histogram of TxnFee', grid=True, \n",
    "                                    title = \"Value of TxnFee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4828f8f-2cae-41d3-99e2-75cc8fd775f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.percentile(df['Value'], percentiles)\n",
    "\n",
    "df['stratum'] = pd.cut(df['Value'], bins=bin_edges, include_lowest=True, labels=False, duplicates = 'drop')\n",
    "\n",
    "sample_size_per_stratum = 5 \n",
    "stratified_sample = df.groupby('stratum', group_keys=False).apply(lambda x: x.sample(min(len(x), sample_size_per_stratum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff7556-3d9f-4eb4-a0c1-6b819c3f933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Mean of Value\" , stratified_sample['Value'].mean())\n",
    "print( \"Std of Value\" , stratified_sample['Value'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e41fa4-fa99-4a51-a39d-2b2c711e9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "df['stratum'] = df['stratum'].astype('category')  # Ensuring 'stratum' is treated as a categorical variable\n",
    "stratified_sample.boxplot(column='Value', by='stratum')\n",
    "plt.title('Value paratmeter Box Plot of Stratified Sample Values by Stratum')\n",
    "plt.suptitle('')  # Remove the default 'Title' subtitle to clean up the plot title\n",
    "plt.xlabel('Stratum')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d2dd9-de9a-47c8-adc7-085de02e4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Value'].plot(kind='hist', bins=(find_bin_size(df['Value'])), density=False, alpha=0.5, label='Histogram of Value', grid=True, \n",
    "                                    title = \"Value of Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ace1a-1cee-4362-a9e2-0fc42d0f286e",
   "metadata": {},
   "source": [
    "- Analysis\n",
    "For comparing Transaction value and TxnFee, both hase skew to the right but in for Transaction Value is higher.\n",
    "In all states , population and samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238496cf-e651-4ff1-9ed4-4b87dfe4bb64",
   "metadata": {},
   "source": [
    "- Compare two sampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9302d91-9a5b-436a-92bb-94d546e46d7c",
   "metadata": {},
   "source": [
    "SRS:\n",
    "1. Simplicity and ease of implementation.\n",
    "2. Generalizations about the entire population without bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a0743-abff-4101-9298-54389598f197",
   "metadata": {},
   "source": [
    "Strarified:\n",
    "1. Greater precision: Stratified samples can provide more accurate estimates than SRS of the same size.\n",
    "2. Cost savings: Often requires a smaller sample size.\n",
    "3. Guards against unrepresentative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112c08a-24fa-4179-bee2-7262f4c416c5",
   "metadata": {},
   "source": [
    "- Compare samples and populations\n",
    "\n",
    "- By comparing plots and mean and std for both of  population and samples, we see stratified is better than SRS because it's values are near to real population's values and is bt=etter extimation.\n",
    "\n",
    "- In stratified sample we can see better visualization for our data because it collect from diffrent parts of data.\n",
    "\n",
    "- Is possible SRS that shows not enough information and lead us astrary.\n",
    "\n",
    "- For transaction value, chance of collecting from zero zone is higher because is more dense over there, and maybe destroy our sampling!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590aa48",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7161b41",
   "metadata": {},
   "source": [
    "## Q1. What are some potential limitations when using web scraping for data collection?Specifically, what problems did you face while fetching data from Etherscan? What problems can these limitations cause in your analysis?\n",
    "Some potential limitations of using web scraping for data collection include:\n",
    "\n",
    "1. **Website Structure Changes**: Websites frequently update their structure, which can break the web scraping code that's built based on the previous structure.\n",
    "\n",
    "2. **Rate Limiting and IP Blocking**: Websites may have restrictions in place to prevent automated scraping, such as rate limits or IP blocking, which can hinder the scraping process.\n",
    "\n",
    "3. **Data Quality Issues**: Data extracted through web scraping may contain errors, missing values, or inconsistencies due to the scraping process.\n",
    "\n",
    "4. **Legal Issues**: Web scraping may violate a website's terms of service or copyright laws if done without permission, leading to potential legal consequences.\n",
    "\n",
    "5. **Complex Data Retrieval**: Some websites may require navigating through multiple pages or forms before reaching the desired data, complicating the scraping process.\n",
    "\n",
    "6. **Dynamic Content**: Websites that heavily rely on JavaScript for content loading may present a challenge for simple scraping methods as the content may not be readily available in the initial page source.\n",
    "\n",
    "When fetching data from Etherscan specifically, some challenges you may face include:\n",
    "\n",
    "1. **API Limitations**: Etherscan may have limits on the number of API requests that can be made within a given time frame, affecting the amount of data you can fetch.\n",
    "\n",
    "2. **Authentication Requirements**: Some data on Etherscan may require authentication or authorization to access, which can complicate the scraping process.\n",
    "\n",
    "3. **Data Formatting**: The data fetched from Etherscan may not be in a structured format, requiring additional processing to make it usable for analysis.\n",
    "\n",
    "4. **Rate Limiting**: Etherscan has rate limits on the number of requests that can be made in a certain time period, leading to potential blocks or delays in data retrieval.\n",
    "\n",
    "These limitations can lead to several issues in data analysis, such as:\n",
    "\n",
    "1. **Incomplete Data**: Failure to retrieve all necessary data due to limitations or errors in scraping can result in incomplete datasets, impacting the reliability of analysis and conclusions drawn from it.\n",
    "\n",
    "2. **Data Integrity**: Changes in the structure or content of scraped data can introduce inconsistencies or errors that may not be immediately apparent, leading to erroneous analysis results.\n",
    "\n",
    "3. **Compliance Concerns**: Some websites have terms of service that prohibit or restrict scraping activities, raising legal and ethical concerns if data is collected without proper authorization.\n",
    "\n",
    "\n",
    "These limitations can cause problems in your analysis by potentially introducing errors or biases in the data, leading to inaccurate conclusions. It's important to be aware of these limitations and take steps to mitigate them, such as monitoring for changes in website structure, respecting rate limits, and ensuring data quality checks are in place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e71ff",
   "metadata": {},
   "source": [
    "## Q2.What can make your analysis untrustworthy? What are your solutions?\n",
    "Several factors can make an analysis untrustworthy, including:\n",
    "\n",
    "1. **Data Quality Issues**: Inaccurate, incomplete, or inconsistent data can lead to unreliable results.\n",
    "\n",
    "2. **Biases in Data Collection**: If the data collected is not representative or is skewed in some way, the analysis may not be reliable.\n",
    "\n",
    "3. **Incorrect Analysis Methods**: Using inappropriate analytical techniques or misinterpreting results can lead to faulty conclusions.\n",
    "\n",
    "4. **Overlooking Variables**: Failing to consider all relevant factors or variables in the analysis can result in incomplete or misleading findings.\n",
    "\n",
    "5. **Biased Data Selection**: If the data used for analysis is not representative or is selectively biased, the conclusions drawn may not accurately reflect the true picture.\n",
    "\n",
    "6. **Unsound Methodology**: Using flawed or inappropriate analytical methods can produce unreliable results, casting doubt on the validity of the analysis.\n",
    "\n",
    "7. **Overfitting**: Fitting a model too closely to the training data can lead to overfitting, where the model performs well on the training data but poorly on new data, reducing its reliability.\n",
    "\n",
    "To ensure the trustworthiness of your analysis, consider the following solutions:\n",
    "\n",
    "1. **Ensure Data Quality**: Clean and preprocess data properly, check for outliers, missing values, and inconsistencies, and verify the accuracy of the data.\n",
    "\n",
    "2. **Use Reliable Sources**: Gather data from reputable and trustworthy sources to minimize the risk of biased or unreliable data.\n",
    "\n",
    "3. **Transparent Methodology**: Clearly document and explain the analytical methods used, including any assumptions and limitations. This transparency allows others to validate the analysis.\n",
    "\n",
    "4. **Validate Results**: Conduct sensitivity analyses or robustness checks to verify the robustness of the results and assess the impact of different assumptions.\n",
    "\n",
    "5. **Peer Review**: Have your analysis reviewed by peers or subject matter experts to gain feedback, identify potential flaws, and improve the quality of the analysis.\n",
    "\n",
    "6. **Sensitivity Analysis**: Conduct sensitivity analysis to assess the impact of variations or uncertainties in the data on the results, providing insights into the robustness of the analysis.\n",
    "\n",
    "7. **Cross-Validation**: Use cross-validation techniques to evaluate model performance on unseen data and prevent overfitting, enhancing the generalizability of results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124fc16c",
   "metadata": {},
   "source": [
    "##  Q3.How did the visualization help you in understanding the data? What could you interpret from the plots?\n",
    "Visualization plays a crucial role in understanding and interpreting data by providing a visual representation of the information. Here are some/ ways in which visualization helped in understanding the data and the potential interpretations from the plots:\n",
    "\n",
    "1. **Identifying Patterns and Trends**: Visualizations such as line graphs or scatter plots can help identify patterns, trends, and relationships within the data that may not be apparent from looking at raw numbers.\n",
    "\n",
    "2. **Comparing Data**: Bar charts, pie charts, or histograms can be used to compare different categories or variables, allowing for easier interpretation of differences or similarities.\n",
    "\n",
    "3. **Detecting Outliers**: Box plots or scatter plots can help identify outliers or anomalies in the data, providing insights into data quality or potential errors.\n",
    "\n",
    "4. **Understanding Distribution**: Histograms or density plots can help visualize the distribution of data, such as whether it is normally distributed or skewed, aiding in statistical analysis.\n",
    "\n",
    "5. **Correlation Analysis**: Scatter plots or correlation matrices can help visualize the relationship between variables, showing if there is a positive, negative, or no correlation between them.\n",
    "\n",
    "6. **Geospatial Insights**: Maps and geospatial visualizations help understand geographic patterns in data, such as regional differences or concentrations.\n",
    "\n",
    "Interpretations from specific types of plots:\n",
    "\n",
    "- **Line Plots**: These can show trends over time or relationships between variables. Steep inclines or declines indicate significant changes.\n",
    "\n",
    "- **Bar Charts**: Useful for comparing categories. Differences in bar lengths indicate variations in quantities.\n",
    "\n",
    "- **Pie Charts**: Show the proportion of different categories in a whole. Larger slices represent more significant portions.\n",
    "\n",
    "- **Scatter Plots**: Help visualize relationships between two variables. Clustering of points or linear patterns can indicate correlations.\n",
    "\n",
    "- **Heatmaps**: Display variations in data using color gradients. Hotspots indicate areas of high values while cooler areas represent lower values.\n",
    "\n",
    "By interpreting the plots generated from the visualization of data, one can draw meaningful insights, make informed decisions, and communicate findings effectively. Visualizations can simplify complex data sets and convey information in a clear and intuitive manner, helping to uncover insights and trends that might not be obvious from looking at the raw data alone.\n",
    "\n",
    "In essence, visualizations aid in gaining insights, facilitating data exploration, and presenting findings effectively. They enhance the understanding of complex datasets and make it easier to communicate insights to stakeholders or decision-makers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03682305",
   "metadata": {},
   "source": [
    "## Q4. How do the two sampling methods differ in their output? Compare these and explain which one is a better fit to the population\n",
    "The two main sampling methods are random sampling and stratified sampling. Here's how they differ in their output and which one may be a better fit for the population:\n",
    "\n",
    "1. **Simple Random Sampling**:\n",
    "   - **Output**: In random sampling, each member of the population has an equal chance of being selected. The sample is chosen entirely by chance, and every individual has the same probability of being included in the sample.\n",
    "   - **Advantages**: Random sampling is straightforward to implement and eliminates bias in the selection process. It is suitable when the population is homogeneous and there are no distinct subgroups.\n",
    "       - It is a simple and straightforward method that does not require prior knowledge of the population's characteristics.\n",
    "       - The sample obtained through random sampling is more likely to be representative of the population as a whole.\n",
    "       - It is suitable when the population is homogenous, and there are no distinct subgroups to consider.\n",
    "    - **Disadvantages**: There is a risk of sampling error, as random sampling may not adequately represent specific subgroups within the population.\n",
    "\n",
    "2. **Stratified Sampling**:\n",
    "   - **Output**: In stratified sampling, the population is divided into homogeneous subgroups (or strata) based on certain characteristics. Samples are then randomly selected from each stratum proportionally to their size in the population.\n",
    "   - **Advantages**: Stratified sampling ensures that each subgroup is represented in the sample, making it more reliable for analyzing specific characteristics within the population. It can provide more precise estimates for subgroups.\n",
    "       - Proportional or disproportionate samples are then taken from each stratum based on their representation in the population.\n",
    "       - It ensures that each subgroup of the population is adequately represented in the sample, making it suitable for heterogeneous populations with distinct subgroups.\n",
    "   - **Disadvantages**: It requires knowledge of the population characteristics to create strata, which might not always be available. It is more complex and time-consuming compared to simple random sampling.\n",
    "   \n",
    "**Comparison**:\n",
    "- Random sampling is ideal when the population is relatively homogenous and there are no subgroups that require specific representation in the sample.\n",
    "- Stratified sampling is preferred when the population is diverse, and it is important to ensure that each subgroup is represented in the sample proportionally to its presence in the population.\n",
    "\n",
    "**Which is a Better Fit?**:\n",
    "The choice between random sampling and stratified sampling depends on the research objectives, the homogeneity of the population, and the availability of information about the population. If the population is relatively homogeneous and there are no distinct subgroups, random sampling may suffice. However, if there are known subgroups with varying characteristics, stratified sampling would be a better fit as it ensures that these subgroups are adequately represented in the sample.\n",
    "\n",
    "In general, stratified sampling is often preferred when the population exhibits significant variability across different characteristics or when researchers are interested in understanding specific subgroups within the population, making it a better fit for such scenarios.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cadd607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
